{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4936d40c",
   "metadata": {},
   "source": [
    "### Guided Computation of Fused Multi-Modal Electron Microscopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d534d7",
   "metadata": {
    "part": "Code Walkthrough"
   },
   "source": [
    "In this tutorial we walkthough how you can fuse your EELS/EDX maps with HAADF or similar elastic imaging modalities to improve chemical resolution. This is Tutorial 1 of 2 where we look at an atomic resolution HAADF and EDX dataset of DyScO$_3$. The multi-modal data fusion workflow relies on Python, and requires minimal user input with <10 tunable lines. Both here and in the Mathematical Overview section we outline best practices for these adjustments.  Within a few minutes, datasets such as the one in this tutorial can be transformed into resolution enhanced chemical maps. (Figure 4.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae2b84",
   "metadata": {},
   "source": [
    ":::{figure} ./figs/Figure_3_Output.png\n",
    ":name: Raw vs Fused DyScO$_3$\n",
    ":width: 700px\n",
    "Comparison of raw input vs fused multi-modal DyScO$_3$ HAADF elastic and EDX inelastic images\n",
    ":::\n",
    "\n",
    "```{warning} Step 0: Experimental Requirements\n",
    "To reconstruct using fused multi-modal electron microscopy you need to collect both elastic (e.g. HAADF) and inelastic (e.g. EELS / EDX) maps of your material. For the elastic signal, it is important that it provides Z-contrast of your elements. For the inelastic signal, you should have all chemistries in your sample mapped. Solving for under-determined chemical maps, or using difficult to interpret elastic signals are outside the scope of this tutorial. The collected chemical maps and HAADF must all have the same dimensionality, i.e. the same image size and number of pixels.  For this reason, we recommend using the simultaneously collected HAADF signal that is collected when taking an EDX/EELS scan.\n",
    "```\n",
    "\n",
    "```{admonition} Step 1: Python Imports\n",
    "First, we import standard python packages alongside our custom class of functions for data fusion called [fusion_utils](https://github.com/jtschwar/Multi-Modal-2D-Data-Fusion/blob/170fea3292da7e6390bfff7236610eb0c8077ff7/EDX/fusion_utils.py). The fusion_utils package contains 3 functions and a class of TV functions: save_data(), plot_convergence(), create_weighted_measurement_matrix(), and tvlib(). The save_data function saves the fused images into .tif files and all the parameters and matrix data into a .h5 file. Plot_convergence plots the convergence of the cost functions. Create_weighted_measurement_matrix is for generating the first part of our cost function that relates the elastic and inelastic modalities. Finally, the class of functions called tvlib performs the Fast Gradient Project (FGP) method of TV image denoising, which deblurs output images while preserving edges.\n",
    "```\n",
    ":::{code-cell} ipython3\n",
    "import data/fusion_utils as utils\n",
    "\n",
    "from scipy.sparse import spdiags\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    ":::\n",
    "\n",
    "```{admonition} Step 2: Load your data\n",
    "Load your inelastic and elastic data. Define the element names and their corresponding atomic weights. For the sake of this tutorial a generic .npz file is used, but for .dm3,.dm4,.emd, or another EM file format, just extract the 2D image data into numpy matrices. In Tutorial 2 this is shown by extracting from a .h5 file. Your elastic data should be stored in the HAADF variable, and your inelastic data (EDX/EELS) should be stored in the chemMap variable.\n",
    "```\n",
    "```{tip} Loading alternate file formats\n",
    "If you are loading data from a .dm3, .dm4 or .emd file, we recommend you use HyperSpy.  The documentation for loading and saving data from those file types can be found [here](https://hyperspy.org/hyperspy-doc/v1.3/user_guide/io.html).\n",
    "```\n",
    "\n",
    ":::{code-cell} ipython3\n",
    "data = np.load('data/PTO_Trilayer_dataset.npz')\n",
    "# Define element names and their atomic weights\n",
    "elem_names=['Sc', 'Dy', 'O']\n",
    "elem_weights=[21,66,8]\n",
    "# Parse elastic HAADF data and inelastic chemical maps based on element index from line above\n",
    "HAADF = data['HAADF']\n",
    "xx = np.array([],dtype=np.float32)\n",
    "for ee in elem_names:\n",
    "\n",
    "\t# Read Chemical Map for Element \"ee\"\n",
    "\tchemMap = data[ee]\n",
    "\n",
    "    # Check if chemMap has the same dimensions as HAADF\n",
    "\tif chemMap.shape != HAADF.shape:\n",
    "\t\traise ValueError(f\"The dimensions of {ee} chemical map do not match HAADF dimensions.\")\n",
    "\t\n",
    "\t# Set Noise Floor to Zero and Normalize Chemical Maps\n",
    "\tchemMap -= np.min(chemMap); chemMap /= np.max(chemMap)\n",
    "\n",
    "\t# Concatenate Chemical Map to Variable of Interest\n",
    "\txx = np.concatenate([xx,chemMap.flatten()])\n",
    ":::\n",
    "\n",
    "```{admonition} Step 3: Reshape your data\n",
    "Run the following cell without changing anything.  This cell reshapes your data and relates the elastic to the inelastic modalities within a cost function, which is described in greater depth in the Mathematical Methods section.\n",
    "```\n",
    "\n",
    "```{danger} Caution!\n",
    "Do not change the code below.\n",
    "```\n",
    "\n",
    ":::{code-cell} ipython3\n",
    "# Make Copy of Raw Measurements for Poisson Maximum Likelihood Term \n",
    "xx0 = xx.copy()\n",
    "\n",
    "# Incoherent linear imaging for elastic scattering scales with atomic number Z raised to γ  ∈ [1.4, 2]\n",
    "gamma = 1.6 \n",
    "\n",
    "# Image Dimensions\n",
    "(nx, ny) = chemMap.shape; nPix = nx * ny\n",
    "nz = len(elem_names)\n",
    "\n",
    "# C++ TV Min Regularizers\n",
    "reg = utils.tvlib(nx,ny)\n",
    "\n",
    "# Data Subtraction and Normalization \n",
    "HAADF -= np.min(HAADF); HAADF /= np.max(HAADF)\n",
    "HAADF=HAADF.flatten()\n",
    "\n",
    "# Create Summation Matrix\n",
    "A = utils.create_weighted_measurement_matrix(nx,ny,nz,elem_weights,gamma,1)\n",
    ":::\n",
    "\n",
    "```{admonition} Optional\n",
    ":class: tip \n",
    "Plot your raw elastic/inelastic data\n",
    "```\n",
    "\n",
    ":::{code-cell} ipython3\n",
    "fig, ax = plt.subplots(2,len(elem_names)+1,figsize=(12,8))\n",
    "ax = ax.flatten()\n",
    "ax[0].imshow(HAADF.reshape(nx,ny),cmap='gray'); ax[0].set_title('HAADF'); ax[0].axis('off')\n",
    "ax[1+len(elem_names)].imshow(HAADF.reshape(nx,ny)[70:130,25:85],cmap='gray'); ax[1+len(elem_names)].set_title('HAADF Cropped'); ax[1+len(elem_names)].axis('off')\n",
    "\n",
    "for ii in range(len(elem_names)):\n",
    "    ax[ii+1].imshow(xx0[ii*(nx*ny):(ii+1)*(nx*ny)].reshape(nx,ny),cmap='gray'); ax[ii+1].set_title(elem_names[ii]); ax[ii+1].axis('off')\n",
    "    ax[ii+2+len(elem_names)].imshow(xx0[ii*(nx*ny):(ii+1)*(nx*ny)].reshape(nx,ny)[70:130,25:85],cmap='gray'); ax[ii+2+len(elem_names)].set_title(elem_names[ii]+' Cropped'); ax[ii+2+len(elem_names)].axis('off')\n",
    "plt.show()\n",
    ":::\n",
    "\n",
    "```{admonition} Step 4: Fine tune your weights for each of the three parts of the cost function \n",
    "The first weight, labeled as lambdaHAADF, luckily does not need tuning since it is ideal as the inverse of the number of elements.  The second weight, labeled as lambdaChem, is our data consistency term and we typically find to be ideal between 0.05 and 0.3 although the total range for this term is 0 to 1. The final weight is lambdaTV and it dictates how strong our Total Variation denoising is. In order to preserve fine features while removing noise, we find that a <0.2 lambdaTV value is ideal. nIter is the number of iterations the cost function will run for. although we typically see convergence within 10 iterations, we recommend using 30 iterations to make sure convergence is met long term. the variable bkg represents the background and we use this number to perform minor background subtraction to try and reduce noise; we recommend keeping this variable small. Finally, we give the user the option to turn the TV regularization off, and define a nIter_TV variable to define the number of iterations the FGP TV algorithm should run for.\n",
    "```\n",
    "\n",
    ":::{code-cell} ipython3\n",
    "# Convergence Parameters\n",
    "lambdaHAADF = 1/nz # Do not modify this\n",
    "lambdaChem = 0.08\n",
    "lambdaTV = 0.15; #Typically between 0.001 and 1\n",
    "nIter = 30 # Typically 10-15 will suffice\n",
    "bkg = 2.4e-1\n",
    "\n",
    "# FGP TV Parameters\n",
    "regularize = True; nIter_TV = 3; \n",
    ":::\n",
    "\n",
    "```{admonition} Step 5: Run the Fused Multi-Modal algorithm\n",
    "The code will typically finsh running in <10 seconds even when runnings for hundreds of iterations.  For most applications, the number of iterations will be <100 and the code should run in under 3 seconds.\n",
    "\n",
    "```\n",
    "\n",
    "```{danger} Caution!\n",
    "Do not change the code below.\n",
    "```\n",
    "\n",
    ":::{code-cell} ipython3\n",
    "# xx represents the flattened 1D elastic maps we are trying to improve via the cost function\n",
    "xx = xx0.copy()\n",
    "\n",
    "# Auxiliary Functions for measuring the cost functions\n",
    "lsqFun = lambda inData : 0.5 * np.linalg.norm(A.dot(inData**gamma) - HAADF) **2\n",
    "poissonFun = lambda inData : np.sum(xx0 * np.log(inData + 1e-8) - inData)\n",
    "\n",
    "# Main Loop\n",
    "\n",
    "# Initialize the three cost functions components \n",
    "costHAADF = np.zeros(nIter,dtype=np.float32); costChem = np.zeros(nIter, dtype=np.float32); costTV = np.zeros(nIter, dtype=np.float32);\n",
    "\n",
    "for kk in tqdm(range(nIter)):\n",
    "\t# Solve for the first two optimization functions $\\Psi_1$ and $\\Psi_2$\n",
    "\txx -=  gamma * spdiags(xx**(gamma - 1), [0], nz*nx*ny, nz*nx*ny) * lambdaHAADF * A.transpose() * (A.dot(xx**gamma) - HAADF) + lambdaChem * (1 - xx0 / (xx + bkg))\n",
    "\n",
    "\t# Enforce positivity constraint\n",
    "\txx[xx<0] = 0\n",
    "\n",
    "\t# FGP Regularization if turned on\n",
    "\tif regularize:\n",
    "\t\tfor zz in range(nz):\n",
    "\t\t\txx[zz*nPix:(zz+1)*nPix] = reg.fgp_tv( xx[zz*nPix:(zz+1)*nPix].reshape(nx,ny), lambdaTV, nIter_TV).flatten()\n",
    "\n",
    "\t\t\t# Measure TV Cost Function\n",
    "\t\t\tcostTV[kk] += reg.tv( xx[zz*nPix:(zz+1)*nPix].reshape(nx,ny) )\n",
    "\t\t\t\n",
    "\t# Measure $\\Psi_1$ and $\\Psi_2$ Cost Functions\n",
    "\tcostHAADF[kk] = lsqFun(xx); costChem[kk] = poissonFun(xx)\n",
    ":::\n",
    "\n",
    "```{admonition} Step 6: Assess Convergence\n",
    "Assess convergence by confirming that all 3 cost functions asymptotically approach a low value as shown in the example plots below.   You should see all three curves start high and approach a plateau without any substantial changes. Sometimes the metric value may appears as a exponential decay and othertimes it may overshoot briefly superficially resembling the shape of a Lennard-Jones potential. Look out for incomplete convergence or severe oscillations. If you do not have convergence, the easiest solution is to run the reconstruction for more iterations or adjust the associated lambda value for that term.  For example, if the lambdaCHEM cost function does not show convergence, try change the lambdaCHEM value to something lower.  Repeat until all 3 cost functions show convergence.\n",
    "```\n",
    "\n",
    ":::{figure} ./figs/Figure_4_Convergence.png\n",
    ":name: convergence\n",
    ":width: 700px\n",
    "Convergence of 3 subparts of the multi-modal cost function.  The top plot represents the first term that is dictated by $\\lambda_{HAADF}$.  The middle plot represents the second term that is dictated by $\\lambda_{Chem}$. The bottom plot represents the third TV term that is dictated by $\\lambda_{TV}$. \n",
    ":::\n",
    "\n",
    "```{admonition} Be careful with $\\lambda_{TV}$!\n",
    ":class: attention\n",
    "You may see convergence with an over or under weighted TV term.  Under weighting the TV term results in noisy reconstructions, while over weighting results in blurring and loss of features in the image.  Below is an example of the output images from under, over, and just right TV weights. Err on the side of under weighting the TV term  because noise is familiar to the data, but when oversmoothing, the added artifacts are unphysical and can lead to faulty interpretations.\n",
    "```\n",
    "\n",
    ":::{figure} ./figs/Figure_5_TV.png\n",
    ":name: TV_weights\n",
    ":width: 700px\n",
    "Comparison of TV weighting across different chemistries and HAADF. Too low of a TV results in noise and artifacts across images.  Proper TV preserves fine features like the dumbell shape of the Dy particles, while reducing noise.  High TV oversmoothes the image resulting in loss of important features for analysis.\n",
    ":::\n",
    "\n",
    ":::{code-cell} ipython3\n",
    "# Display Cost Functions and Descent Parameters\n",
    "utils.plot_convergence(costHAADF, lambdaHAADF, costChem, lambdaChem, costTV, lambdaTV)\n",
    "# Show Reconstructed Signal\n",
    "fig, ax = plt.subplots(2,len(elem_names)+1,figsize=(12,8))\n",
    "ax = ax.flatten()\n",
    "ax[0].imshow((A.dot(xx**gamma)).reshape(nx,ny),cmap='gray'); ax[0].set_title('HAADF'); ax[0].axis('off')\n",
    "ax[1+len(elem_names)].imshow((A.dot(xx**gamma)).reshape(nx,ny)[70:130,25:85],cmap='gray'); ax[1+len(elem_names)].set_title('HAADF Cropped'); ax[1+len(elem_names)].axis('off')\n",
    "\n",
    "for ii in range(len(elem_names)):\n",
    "    ax[ii+1].imshow(xx[ii*(nx*ny):(ii+1)*(nx*ny)].reshape(nx,ny),cmap='gray'); ax[ii+1].set_title(elem_names[ii]); ax[ii+1].axis('off')\n",
    "    ax[ii+2+len(elem_names)].imshow(xx[ii*(nx*ny):(ii+1)*(nx*ny)].reshape(nx,ny)[70:130,25:85],cmap='gray'); ax[ii+2+len(elem_names)].set_title(elem_names[ii]+' Cropped'); ax[ii+2+len(elem_names)].axis('off')\n",
    "plt.show()\n",
    ":::\n",
    "\n",
    "```{admonition} Step 7: Save your data\n",
    "Define the folder you would like to save to. The output images and data will be saved to .tif and .h5 file formats\n",
    "```\n",
    "\n",
    ":::{code-cell} ipython3\n",
    "save_folder_name='test'\n",
    "utils.save_data(save_folder_name, xx0, xx, HAADF, A.dot(xx**gamma), elem_names, nx, ny, costHAADF, costChem, costTV, lambdaHAADF, lambdaChem, lambdaTV, gamma)\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "numbering": {
   "enumerator": "4.%s"
  },
  "source_map": [
   14,
   17,
   19
  ],
  "title": "Multi-Modal Tutorial Dataset 1"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}