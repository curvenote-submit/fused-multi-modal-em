%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{booktabs}{sphinx}
\PassOptionsToPackage{colorrows}{sphinx}

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]



\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{My Jupyter Book}
\date{Jun 28, 2024}
\release{}
\author{The Jupyter Book community}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}
\sphinxAtStartPar
Chemical mapping of nano\sphinxhyphen{}scale and atomic chemistry via spectroscopic electron microscopy has challenged researchers for decades due to the fundamental tradeoff between electron dose, noise, and resolution. Inelastic spectroscopic signals (electron energy loss (EELS) or energy\sphinxhyphen{}dispersive x\sphinxhyphen{}ray (EDX) spectroscopy) that yield elemental signatures are rare, resulting in long dwell times to produce discernable structure. This process often destroys the sample and distorts the chemical map during acquisition.  Lower dose scans can be taken to combat this issue, but they often do not have enough signal to produce meaningful results.

\sphinxAtStartPar
Fused multi\sphinxhyphen{}modal electron microscopy offers nano\sphinxhyphen{} and atomic\sphinxhyphen{}resolution high signal\sphinxhyphen{}to\sphinxhyphen{}noise\sphinxhyphen{}ratio (SNR) recovery of material chemistry by merging low\sphinxhyphen{}dose elastically scattered high\sphinxhyphen{}angle annular dark\sphinxhyphen{}field (HAADF) signals with the inelastic signals (\sphinxhref{https://doi.org/10.1038/s41524-021-00692-5}{Schwartz 2022}) (\sphinxhref{https://doi.org/10.1038/s41467-024-47558-0}{Schwartz 2024}). The process works best when all chemistries in the material are mapped spectroscopically and combined with a directly interpretable elastic signal. This article bridges the gap between theory and practice by walking through each step of the fused multi\sphinxhyphen{}modal computational pipeline and pointing out best practices when running the code. By the end of this tutorial, anyone with inelastic and elastic 2D projection data should be able to reproducibly  reconstruct chemical maps using fused multi\sphinxhyphen{}modal electron microscopy.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{Figure_abstract1}.png}
\caption{Fused Multi\sphinxhyphen{}Modal atomic resolution image improvement on  DyScO\(_3\)}\label{\detokenize{index:fig-abstract}}\end{figure}



\sphinxAtStartPar
We acknowledge support from Dow Chemical Company.

\sphinxAtStartPar
Work at the Molecular Foundry was supported by the Office of Basic Energy Sciences, of the U.S. Department of Energy under Contract no. DE\sphinxhyphen{}AC02\sphinxhyphen{}05CH11231.

\sphinxAtStartPar
This research used resources of the Argonne Leadership Computing Facility, a U.S. Department of Energy (DOE) Office of Science user facility at Argonne National Laboratory and is based on research supported by the U.S. DOE Office of Science\sphinxhyphen{}Advanced Scientific Computing Research Program, under Contract No. DE\sphinxhyphen{}AC02\sphinxhyphen{}06CH11357.

\sphinxAtStartPar
This research used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE\sphinxhyphen{}AC05\sphinxhyphen{}00OR22725.

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\Large Competing Interests}
\end{DUlineblock}

\sphinxAtStartPar
The authors declare that they have no known competing interests.

\sphinxstepscope

\sphinxAtStartPar
Scanning transmission electron microscopes (STEM) can enable the precise quantification of structure and chemistry through the collection and analysis of elastic and inelastic scattering events. The modern electron beam can be tuned to lateral dimensions less than than an angstrom and scanned between and across atoms. Collecting inelastic scattering events and at every energy loss using either energy dispersive x\sphinxhyphen{}ray (EDX) or electron energy loss spectroscopy (EELS) has allowed routine measurement of chemistry at the atomic length\sphinxhyphen{}scale. However, the acquisition of high\sphinxhyphen{}resolution chemical images often necessitates doses that may surpass the tolerance of the specimen, leading to chemical maps that are either noisy or completely absent (\sphinxhref{https://doi.org/10.1038/s41598-017-07709-4}{Hart 2017}).  Challenges in this field of high resolution chemical imaging remain prevalent despite advancements in detector effiency(\sphinxhref{https://doi.org/10.1016/j.ultramic.2014.08.002}{McMullan 2014}) (\sphinxhref{https://doi.org/10.1017/s143192762101360x}{Zaluzec 2022}) (\sphinxhref{https://doi.org/10.48550/arXiv.2007.09747}{Goodge 2020}). The HAADF detector collects elastically scattered electrons, offering a direct view of the atomic structure with a higher SNR, but provides limited chemical information. Some chemical information is inferred from HAADF projections since the elastic scattering off an element is roughly proportional to atomic number \(Z^2\) (\sphinxhref{https://www.science.org/doi/10.1126/science.168.3937.1338}{Crew 1970}) (\sphinxhref{https://doi.org/10.1016/0304-3991(96)00020-4}{Rose 1996}) (\sphinxhref{https://doi.org/10.1103/PhysRevLett.100.206101}{LeBeau 2008}) (\sphinxhref{https://doi.org/10.1016/j.ultramic.2012.04.014}{Hovden 2012}). However, solving chemistry from HAADF is often difficult or impossible when mulitple chemistries are present, two elements are close in atomic number, or there are large variations in material density. Additionally, since the image taken on the microscope is a projection through the whole viewing area, differential density further complicates how to interpret different sections in the image and leads to false conclusions.

\sphinxAtStartPar
Traditionally, detector outputs like HAADF and EDX/EELS have been analyzed independently, overlooking the potential insights from integrating structure and chemical data from the two seperate modalities. Unlike correlative imaging, data\sphinxhyphen{}fusion merges sparse signals to enhance the accuracy of both measurements (\sphinxhref{https://doi.org/10.1109/5.554205}{Hall 1997}) (\sphinxhref{https://doi.org/10.1109/JPROC.2015.2460697}{Lahat 2015}) (\sphinxhref{https://doi.org/10.1137/15M1021404}{Di 2016}). Due to its power to improve image quality, this data fusion approach has seen an explosion in popularity in the past decade in the panchromatic, infrared, hyperspectral and Lidar satellite remote sensing communities (\sphinxhref{https://doi.org/10.1007/s11432-022-3588-0}{Sun 2023}).  Data fusion relies on crafting a model that can capture interactions between different datasets without forcing unrelated connections leading to false conclusions(\sphinxhref{https://doi.org/10.1016/j.bpsc.2015.12.005}{Calhoun 2016}).

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{Figure_0_comparison}.png}
\caption{HAADF, EDX, and Fused Multi\sphinxhyphen{}Modal comparison of final images collected of DyScO\(_3\)}\label{\detokenize{01_intro:fig-overview}}\end{figure}

\sphinxAtStartPar
We walkthrough the steps for multi\sphinxhyphen{}modal electron microscopy as a mathematical model, a computational algorithm, and finally through code implementation to make this technique more accessible to the greater electron microscopy community.  We have provided a sample dataset that is accompanied by a full code tutorial, and we have provided an algorithm section where every variable is explained and sequentially manipulated for full transparency. This approach has been tested extensively in the case on 2D projections utilizing EDX and EELS signals (\sphinxhref{https://doi.org/10.1038/s41524-021-00692-5}{Schwartz 2022}). When multi\sphinxhyphen{}modal data fusion performs well, it significantly boosts the SNR of chemical maps by 300\sphinxhyphen{}500\% and enables a reduction in required doses by more than an order of magnitude, aligning with the original data. When looking at known synthetic materials we have consistently found accurate chemical mapping and since we can determine relative concentrations within specimens we also have tested stoichiometry and found <15\% error with no known priors of inelastic cross sections. Through this work we hope that within minutes you can go from raw data off the microscope to accurate, publishable fused multi\sphinxhyphen{}modal chemical maps.

\sphinxstepscope

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{figure_1_math}.png}
\caption{Overview of Fused Multi\sphinxhyphen{}Modal pipeline on an atomic\sphinxhyphen{}res DyScO\(_3\) STEM/EDX dataset}\label{\detokenize{02_math:fig-math}}\end{figure}


\chapter{Cost function}
\label{\detokenize{02_math:cost-function}}\label{\detokenize{02_math::doc}}
\sphinxAtStartPar
Fused multi\sphinxhyphen{}modal electron microscopy solves an optimization problem to recover micro\sphinxhyphen{} and nanoscale material chemistry. Solutions to the optimization problem are sought that encourage sparsity in the gradient domain while correlating high SNR HAADF projections with elementally sensitive EELS and/or EDX maps. These solutions ensure reduced spatial variation and accurate elemental mapping. The overall optimization function is framed as an inverse problem taking the following form:
\begin{equation*}
\begin{split}
\hat{x} = \arg\min_{x\geq 0} \left( \Psi_1(x) + \lambda_1 \Psi_2(x) + \lambda_2 TV(x) \right),
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(\hat{x}\) is the final reconstruction, \(\Psi_1\), \(\Psi_2\), \(TV\) are each optimization functions, and \(\lambda\) are their respective weights.

\sphinxAtStartPar
Each term is made up of standard optimization operations which is best illustrated by the full expression:
\begin{equation*}
\begin{split}
\arg \min_{x_i \geq 0} \frac{1}{2} \left\| b_H - \sum_i (Z_i x_i)^\gamma \right\|_2^2 
+ \lambda_1 \sum_i (1^T x_i - b_i^T \log(x_i + \epsilon))
+ \lambda_2 \sum_i \| x_i \|_{TV} 
\end{split}
\end{equation*}
\sphinxAtStartPar
Breaking down each variable, \(b_i\) is the measured HAADF, \(b_j\) and \(x_i\) are the measured and reconstructed chemical maps for element \(i\), \(\epsilon\) is a number close to 0 that can account for the background and prevents \(\log(0)\), \(\log\) is applied element\sphinxhyphen{}wise to its arguments, superscript \(T\) denotes vector transpose, and \(\mathbf{1}\) denotes the vector of \(n_x \times n_y\) ones, where \(n_x \times n_y\) is the image size. When implementing an algorithm to solve this problem, we concatenate the multi\sphinxhyphen{}element spectral variables \((x_i, b_i)\) as a single vector: \(x, b \in \mathbb{R}^{n_x \times n_y \times n_i}\), where \(n_i\) denotes the total number of reconstructed elements.

\sphinxAtStartPar
Equation 1.2 can be broken down into three key terms. The first term is our assumption of a forward model where HAADF projections can be described as a linear combination of elemental distributions raised to the power γ ∈ {[}1.4, 2{]} (\sphinxhref{https://doi.org/10.1016/0304-3991(96)00020-4}{Hartel 1996}). Incoherent linear imaging for high\sphinxhyphen{}angle elastic scattering scales with atomic number Z raised to γ. The second term ensures that any recovered signals maintain data fideltiy with the initial inputs (raw measurements).  A maximum negative log\sphinxhyphen{}likelihood for elemental maps dominated by low\sphinxhyphen{}count Poisson statistics is employed to enforce this data fidelty (\sphinxhref{https://doi.org/10.1364/OE.25.013107}{Di 2017}). The final term is common channel\sphinxhyphen{}wise total variation (\(TV\)) regularization (\sphinxhref{https://doi.org/10.1364/OE.25.013107}{Rudin 1992}). \(TV\) reduces noise by prioritizing the preservation of sharp features. Each of these three terms has a weight attributed that must be balanced to ensure convergance and accurate elemental recovery.

\sphinxAtStartPar
By descending along with the negative gradient directions for the first two terms and subsequently evaluate the isotropic TV proximal operator to denoise the chemical maps we can solve the cost function ({\hyperref[\detokenize{02_math:10.1109/TIP.2009.2028250}]{\sphinxcrossref{\DUrole{xref,myst}{Beck 2009}}}}). The first two term gradients are:
\$\(
\nabla_x \Psi_1(x) = -\gamma \text{diag} \left( (x^\gamma - 1) \right) A^T \left( b_H - Ax^\gamma \right)
\)\$
\begin{equation*}
\begin{split}
\nabla_x \Psi_2(x) = 1 - b \circ (x + \epsilon)
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(\circ\) denotes point\sphinxhyphen{}wise division. Here, the first term in the cost function, relating the elastic and inelastic modalities, has been equivalently re\sphinxhyphen{}written as \(\Psi_1 = \frac{1}{2} \left\| b_H - Ax^\gamma \right\|^2\), where \(A \in \mathbb{R}^{n_x \times n_y \times n_{x} \times n_{y} \times n_i}\) expresses the summation of all elements as matrix–vector multiplication. Evaluation for the TV proximal operator is in itself another iterative algorithm. In addition, we impose a non\sphinxhyphen{}negativity constraint since negative concentrations are unrealistic. We initialize the first iterate with the measured data \((x^0_i = b_i)\), an ideal starting point as it is a local minima for \(\Psi_2\).

\sphinxAtStartPar
\(\frac{1}{L}\) is the reciprocal of the lattice constant \(L\) and serves as a theoretical upper limit for the step size, ensuring possible convergence. Through the principle of Lipschitz continuity, the step size related to the gradient of the model term \(\nabla \Psi_1\) was determined to be no greater than \(\frac{1}{(| A |1 | A |{\infty})} = \frac{1}{n_i}\). The gradient associated with the Poisson negative log\sphinxhyphen{}likelihood, \(\Psi_2\), does not adhere to Lipschitz continuity.  This means that it is impossible to render its descent parameter ahead of time ({\hyperref[\detokenize{02_math:10.1109/TIP.2008.2008223}]{\sphinxcrossref{\DUrole{xref,myst}{Dupe 2009}}}}).


\chapter{Total Variation}
\label{\detokenize{02_math:total-variation}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{Figure_2_TV_Comparison}.png}
\caption{Comparison of GP and FGP convergence on a Fused Multi\sphinxhyphen{}Modal dataset showing how FGP converges must faster than GP}\label{\detokenize{02_math:fig-tv}}\end{figure}

\sphinxAtStartPar
TV minimization (\sphinxhref{https://doi.org/10.1364/OE.25.013107}{Rudin 1992}), regularizes images while preserving sharp features, denoising, and deblurring. Since TV efficiently retains edges during denoising, it is an ideal candidate for electron microscope projection processing due to the low SNR environment often encountered when trying to image materials. TV minimization involves solving a complex problem where the norm actively penalizes the solution, thus ensuring smoothness everywhere except at sharp edges.  A regularization parameter is also utilized to balance fata fidelity with the observed data to the noise inherent in the image.

\sphinxAtStartPar
Traditionally, the Gradient Projection (GP) method has been used to solve for TV minimization, but recent advancements in optimization have introduced a faster convergence method, FGP. First introduced by {\hyperref[\detokenize{02_math:10.1109/TIP.2009.2028250}]{\sphinxcrossref{\DUrole{xref,myst}{Rudin and Teboulle in 2009}}}}, FGP boasts computational efficiency with only minimal additions to standard Gradient Projection steps.  Just like GP, FGP can handle both isotropic and anistropic TV functions making it an ideal candidate for various image processing scenarios.  In terms of convergence rates, FGP exhibits a \(O\left(\frac{1}{k^2}\right)\) rate of convergence instead of \(O\left(\frac{1}{k}\right)\) found in GP. By implementing FGP we have seen at least 10x improvement in convergence speed in our code allowing for hundreds of iterations to be run in under 30 seconds on a Mac laptop. This implementation of FGP allows for parameter fine tuning to be done in minites allowing for Fused MM results from raw data in a single short post\sphinxhyphen{}processing session.

\sphinxstepscope


\chapter{Guided Computation of Fused Multi\sphinxhyphen{}Modal Electron Microscopy}
\label{\detokenize{03_multi_modal:guided-computation-of-fused-multi-modal-electron-microscopy}}\label{\detokenize{03_multi_modal::doc}}
\sphinxAtStartPar
In this tutorial we walkthough how you can fuse your EELS/EDX maps with HAADF or similar elastic imaging modalities to improve chemical resolution. This is Tutorial 1 of 2 where we look at an atomic resolution HAADF and EDX dataset of DyScO\(_3\). The multi\sphinxhyphen{}modal data fusion workflow relies on Python, and requires minimal user input with <10 tunable lines. Both here and in the Mathematical Overview section we outline best practices for these adjustments.  Within a few minutes, datasets such as the one in this tutorial can be transformed into resolution enhanced chemical maps. (Figure 4.1)

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{Figure_3_Output}.png}
\caption{Comparison of raw input vs fused multi\sphinxhyphen{}modal DyScO\(_3\) HAADF elastic and EDX inelastic images}\label{\detokenize{03_multi_modal:raw-vs-fused-dysco-3}}\end{figure}

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
Step 0: Experimental Requirements
To reconstruct using fused multi\sphinxhyphen{}modal electron microscopy you need to collect both elastic (e.g. HAADF) and inelastic (e.g. EELS / EDX) maps of your material. For the elastic signal, it is important that it provides Z\sphinxhyphen{}contrast of your elements. For the inelastic signal, you should have all chemistries in your sample mapped. Solving for under\sphinxhyphen{}determined chemical maps, or using difficult to interpret elastic signals are outside the scope of this tutorial. The collected chemical maps and HAADF must all have the same dimensionality, i.e. the same image size and number of pixels.  For this reason, we recommend using the HAADF signal that is simultaneously collected when taking an EDX/EELS scan
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Step 1: Python Imports}

\sphinxAtStartPar
First, we import standard python packages alongside our custom class of functions for data fusion called \sphinxhref{https://github.com/jtschwar/Multi-Modal-2D-Data-Fusion/blob/170fea3292da7e6390bfff7236610eb0c8077ff7/EDX/fusion\_utils.py}{fusion\_utils}. The fusion\_utils package contains 3 functions and a class of TV functions: save\_data(), plot\_convergence(), create\_weighted\_measurement\_matrix(), and tvlib(). The save\_data function saves the fused images into .tif files and all the parameters and matrix data into a .h5 file. Plot\_convergence plots the convergence of the cost functions. Create\_weighted\_measurement\_matrix is for generating the first part of our cost function that relates the elastic and inelastic modalities. Finally, the class of functions called tvlib performs the Fast Gradient Project (FGP) method of TV image denoising, which deblurs output images while preserving edges.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Step 2: Load your data}

\sphinxAtStartPar
Load your inelastic and elastic data. Define the element names and their corresponding atomic weights. For the sake of this tutorial a generic .npz file is used, but for .dm3,.dm4,.emd, or another EM file format, just extract the 2D image data into numpy matrices. In Tutorial 2 this is shown by extracting from a .h5 file. Your elastic data should be stored in the HAADF variable, and your inelastic data (EDX/EELS) should be stored in the chemMap variable.
\end{sphinxadmonition}

\begin{sphinxadmonition}{tip}{Tip:}
\sphinxAtStartPar
Loading alternate file formats
If you are loading data from a .dm3, .dm4 or .emd file, we recommend you use HyperSpy.  The documentation for loading and saving data from those file types can be found \sphinxhref{https://hyperspy.org/hyperspy-doc/v1.3/user\_guide/io.html}{here}.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Step 3: Reshape your data}

\sphinxAtStartPar
Run the following cell without changing anything.  This cell reshapes your data and relates the elastic to the inelastic modalities within a cost function, which is described in greater depth in the Mathematical Methods section.
\end{sphinxadmonition}

\begin{sphinxadmonition}{danger}{Danger:}
\sphinxAtStartPar
Caution!
Do not change the code below.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Optional}

\sphinxAtStartPar
Plot your raw elastic/inelastic data
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Step 4: Fine tune your weights for each of the three parts of the cost function}

\sphinxAtStartPar
The first weight, labeled as lambdaHAADF, is defined as the inverse of the number of elements to normalize each element’s contribution. This prevents a single element from disproportionately influencing the solution.  The second weight, labeled as lambdaChem, is our data consistency term and we typically find to be ideal between 0.05 and 0.3 although the total range for this term is 0 to 1. The final weight is lambdaTV and it dictates how strong our Total Variation denoising is. In order to preserve fine features while removing noise, we find that a <0.2 lambdaTV value is ideal. nIter is the number of iterations the cost function will run for. although we typically see convergence within 10 iterations, we recommend using 30 iterations to make sure convergence is met long term. the variable bkg represents the background and we use this number to perform minor background subtraction to try and reduce noise; we recommend keeping this variable small. Finally, we give the user the option to turn the TV regularization off, and define a nIter\_TV variable to define the number of iterations the FGP TV algorithm should run for.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Step 5: Run the Fused Multi\sphinxhyphen{}Modal algorithm}

\sphinxAtStartPar
The code will typically finsh running in <10 seconds even when runnings for hundreds of iterations.  For most applications, the number of iterations will be <100 and the code should run in under 3 seconds.
\end{sphinxadmonition}

\begin{sphinxadmonition}{danger}{Danger:}
\sphinxAtStartPar
Caution!
Do not change the code below.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Step 6: Assess Convergence}

\sphinxAtStartPar
Assess convergence by confirming that all 3 cost functions asymptotically approach a low value as shown in the example plots below.   You should see all three curves start high and approach a plateau without any substantial changes. Sometimes the metric value may appear as a exponential decay and othertimes it may overshoot briefly superficially resembling the shape of a Lennard\sphinxhyphen{}Jones potential. Look out for incomplete convergence or severe oscillations. If you do not have convergence, the easiest solution is to run the reconstruction for more iterations or adjust the associated lambda value for that term.  For example, if the lambdaCHEM cost function does not show convergence, try change the lambdaCHEM value to something lower.  Repeat until all 3 cost functions show convergence.
\end{sphinxadmonition}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{Figure_4_Convergence}.png}
\caption{Convergence of 3 subparts of the multi\sphinxhyphen{}modal cost function.  The top plot represents the first term that is dictated by \(\lambda_{HAADF}\).  The middle plot represents the second term that is dictated by \(\lambda_{Chem}\). The bottom plot represents the third TV term that is dictated by \(\lambda_{TV}\).}\label{\detokenize{03_multi_modal:convergence}}\end{figure}

\begin{sphinxadmonition}{note}{Be careful with \protect\(\lambda_{TV}\protect\)!}

\sphinxAtStartPar
You may see convergence with an over or under weighted TV term.  Under weighting the TV term results in noisy reconstructions, while over weighting results in blurring and loss of features in the image.  Below is an example of the output images from under, over, and just right TV weights. Err on the side of under weighting the TV term  because noise is familiar to the data, but when oversmoothing, the added artifacts are unphysical and can lead to faulty interpretations.
\end{sphinxadmonition}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{Figure_5_TV}.png}
\caption{Comparison of TV weighting across different chemistries and HAADF. Too low of a TV results in noise and artifacts across images.  Proper TV preserves fine features like the dumbell shape of the Dy particles, while reducing noise.  High TV oversmoothes the image resulting in loss of important features for analysis.}\label{\detokenize{03_multi_modal:tv-weights}}\end{figure}

\begin{sphinxadmonition}{note}{Step 7: Save your data}

\sphinxAtStartPar
Define the folder you would like to save to. The output images and data will be saved to .tif and .h5 file formats
\end{sphinxadmonition}

\sphinxstepscope


\chapter{Guided Computation of Fused Multi\sphinxhyphen{}Modal Electron Microscopy}
\label{\detokenize{04_multi_modal_2:guided-computation-of-fused-multi-modal-electron-microscopy}}\label{\detokenize{04_multi_modal_2::doc}}
\sphinxAtStartPar
This tutorial is almost identical to the last, but now we use a new Co\(_3\)O\(_4\)\sphinxhyphen{}Mn\(_3\)O\(_4\) dataset which utilizes EELS instead of EDX. We also read from a .h5 file in a similar fashion to how one would read from a .dm3, .dm4, or .emd file format. The parameters for convergence have also changed slightly, highlighting how one set of weights may not work across datasets, hence assessing cost function convergence and regularization weighting is key. Just like the previous dataset, dramatic improvement in image quality is observed within just a few minutes of parameter tuning as seen in Figure 5.1

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{Figure_6_Output_2}.png}
\caption{Comparison of raw input vs fused multi\sphinxhyphen{}modal Co\(_3\)O\(_4\)\sphinxhyphen{}Mn\(_3\)O\(_4\) HAADF elastic and EDX inelastic images}\label{\detokenize{04_multi_modal_2:raw-vs-fused-co-3-o-4-mn-3-o-4}}\end{figure}

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
Step 0: Experimental Requirements
To reconstruct using fused multi\sphinxhyphen{}modal electron microscopy you need to collect both elastic (e.g. HAADF) and inelastic (e.g. EELS / EDX) maps of your material. For the elastic signal, it is important that it provides Z\sphinxhyphen{}contrast of your elements. For the inelastic signal, you should have all chemistries in your sample mapped. Solving for under\sphinxhyphen{}determined chemical maps, or using difficult to interpret elastic signals is outside the scope of this tutorial.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Step 1}

\sphinxAtStartPar
Python Imports
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Step 2}

\sphinxAtStartPar
For this example, the dataset is stored in a .h5 file so this is how you can extract data and then save it numpy arrays for the fused multi\sphinxhyphen{}modal workflow.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Step 3}

\sphinxAtStartPar
Reshape your data
\end{sphinxadmonition}

\begin{sphinxadmonition}{danger}{Danger:}
\sphinxAtStartPar
Caution!
Do not change the code below.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Optional}

\sphinxAtStartPar
Plot your raw elastic/inelastic data
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Step 4}

\sphinxAtStartPar
Fine tune your weights for each of the three parts of the cost function.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Step 5}

\sphinxAtStartPar
Run the Fused Multi\sphinxhyphen{}Modal algorithm. Here an extra line was added to subtract off some of the constant background noise.  This can be helpful when inelastic maps are exceptionally noisy.  The ideal value to background subtract off was found by looking at the mean value of the image found in regions of the image where clear material structure was not present
\end{sphinxadmonition}

\begin{sphinxadmonition}{danger}{Danger:}
\sphinxAtStartPar
Caution!
Do not change the code below.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Step 6}

\sphinxAtStartPar
Assess convergence by confirming that all 3 cost functions asymptotically approach a low value.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Step 7}

\sphinxAtStartPar
Save your data
\end{sphinxadmonition}

\sphinxstepscope


\chapter{Algorithms}
\label{\detokenize{05_algorithms:algorithms}}\label{\detokenize{05_algorithms::doc}}
\sphinxstepscope


\chapter{Summary}
\label{\detokenize{06_conclusion:summary}}\label{\detokenize{06_conclusion:id1}}\label{\detokenize{06_conclusion::doc}}
\sphinxAtStartPar
Here, we presented a tutorial to introduce scientists to fused multi\sphinxhyphen{}modal electron microscopy; a technique that takes two separate signal types from the electron microscope and combines them to leverage helpful information embedded in each. By improving conventional spectroscopic elemental imaging techniques, crucial information on material chemistry, from performance to structure, can be elucidated for various applications. In improving elemental SNR, we also have opened the possibility of lower dose elemental imaging of radiation\sphinxhyphen{}sensitive materials such as polymers and biomaterials.  This tutorial lowers the barrier to entry for scientists without coding, optimization, or mixed modality backgrounds by streamlining the learning process.  This tutorial and the accompanying fused multi\sphinxhyphen{}modal code are easily customizable with minimal user input for fine\sphinxhyphen{}tuning the algorithm for optimal convergence.


\chapter{Future Outlook}
\label{\detokenize{06_conclusion:future-outlook}}\label{\detokenize{06_conclusion:future}}
\sphinxAtStartPar
Despite the validation in this work on EDX and EELS datasets, this data fusion approach can extend to ptychography, 4D\sphinxhyphen{}STEM, low\sphinxhyphen{}loss EELS, valence EELS, and other advanced modalities. As “advanced” techniques become commonplace alongside standard imaging, we expect more signals to be integrated into the fused multi\sphinxhyphen{}modal pipeline, allowing for an unprecedented understanding of nano\sphinxhyphen{} and atomic\sphinxhyphen{}scale materials. Additionally, the recent advancements in liquid helium cooling capabilities provide additional dose protection for current radiation\sphinxhyphen{}sensitive materials so that a cryo\sphinxhyphen{}fused multi\sphinxhyphen{}modal approach opens up the possibility of high\sphinxhyphen{}resolution chemical imaging in soft matter and other biological systems.







\renewcommand{\indexname}{Index}
\printindex
\end{document}